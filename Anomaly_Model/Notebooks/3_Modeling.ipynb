{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0114e1ba-5e25-4bbb-add7-c239cdf91c5b",
   "metadata": {},
   "source": [
    "# Modeling Notebook\n",
    "\n",
    "In this last phase, we'll load the datasets prepared in the Feature Engineering phase, and implement train, and test various anomaly detection machine learning algorithms. Subsequently, we'll evaluate the performance of each algorithm by computing relevant metrics. This process will aid us in selecting the most suitable model for our specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dbeba6-a5d4-450e-a27c-1a1a2c3c44b8",
   "metadata": {},
   "source": [
    "#### Import libraries section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8db4054b-0cfa-41b3-8b7d-248823e559a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dda57f-4bd5-44a7-a5f6-4ed83d770b75",
   "metadata": {},
   "source": [
    "### 1. Load the processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f2c49b-0588-4e24-9dec-21eae13cbc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_df = pd.read_csv(\"../Data/Processed/processed_consolidated_data.csv\")\n",
    "filtered_df = pd.read_csv(\"../Data/Processed/processed_filtered_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9bdf2f8-d79e-4bb0-9944-d3279f1f1ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_6</th>\n",
       "      <th>cluster_10</th>\n",
       "      <th>cluster_15</th>\n",
       "      <th>cluster_14</th>\n",
       "      <th>cluster_4</th>\n",
       "      <th>cluster_2</th>\n",
       "      <th>cluster_17</th>\n",
       "      <th>cluster_8</th>\n",
       "      <th>cluster_12</th>\n",
       "      <th>cluster_3</th>\n",
       "      <th>cluster_9</th>\n",
       "      <th>cluster_5</th>\n",
       "      <th>cluster_1</th>\n",
       "      <th>cluster_13</th>\n",
       "      <th>cluster_0</th>\n",
       "      <th>cluster_11</th>\n",
       "      <th>cluster_7</th>\n",
       "      <th>cluster_16</th>\n",
       "      <th>anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5554</th>\n",
       "      <td>-0.246869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.046225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.103407</td>\n",
       "      <td>-0.696322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.147162</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.244415</td>\n",
       "      <td>-0.130097</td>\n",
       "      <td>0.048848</td>\n",
       "      <td>0.403916</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5555</th>\n",
       "      <td>2.201774</td>\n",
       "      <td>3.381788</td>\n",
       "      <td>3.217840</td>\n",
       "      <td>3.592778</td>\n",
       "      <td>0.651350</td>\n",
       "      <td>3.189005</td>\n",
       "      <td>3.068466</td>\n",
       "      <td>0.389782</td>\n",
       "      <td>1.660412</td>\n",
       "      <td>0.565051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.14595</td>\n",
       "      <td>2.688592</td>\n",
       "      <td>-1.166791</td>\n",
       "      <td>1.996624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.351831</td>\n",
       "      <td>1.770652</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5556</th>\n",
       "      <td>0.224350</td>\n",
       "      <td>2.113636</td>\n",
       "      <td>3.117679</td>\n",
       "      <td>2.208783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.630917</td>\n",
       "      <td>1.342173</td>\n",
       "      <td>0.373364</td>\n",
       "      <td>0.389803</td>\n",
       "      <td>1.290899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.511750</td>\n",
       "      <td>2.654875</td>\n",
       "      <td>1.268348</td>\n",
       "      <td>2.376257</td>\n",
       "      <td>-0.130018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5557</th>\n",
       "      <td>0.010262</td>\n",
       "      <td>1.320406</td>\n",
       "      <td>3.001764</td>\n",
       "      <td>1.361922</td>\n",
       "      <td>-0.600823</td>\n",
       "      <td>0.828067</td>\n",
       "      <td>1.725537</td>\n",
       "      <td>-0.074663</td>\n",
       "      <td>0.726163</td>\n",
       "      <td>1.130658</td>\n",
       "      <td>-0.126752</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>-1.225484</td>\n",
       "      <td>0.491605</td>\n",
       "      <td>1.876478</td>\n",
       "      <td>-4.492730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5558</th>\n",
       "      <td>0.400114</td>\n",
       "      <td>-0.751519</td>\n",
       "      <td>2.053032</td>\n",
       "      <td>0.364424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.149691</td>\n",
       "      <td>2.461613</td>\n",
       "      <td>1.128856</td>\n",
       "      <td>0.314571</td>\n",
       "      <td>-0.373194</td>\n",
       "      <td>0.129812</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.696582</td>\n",
       "      <td>2.329912</td>\n",
       "      <td>-0.063248</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.082809</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cluster_6  cluster_10  cluster_15  cluster_14  cluster_4  cluster_2  \\\n",
       "5554  -0.246869         NaN         NaN         NaN  -0.046225        NaN   \n",
       "5555   2.201774    3.381788    3.217840    3.592778   0.651350   3.189005   \n",
       "5556   0.224350    2.113636    3.117679    2.208783   0.000000   2.630917   \n",
       "5557   0.010262    1.320406    3.001764    1.361922  -0.600823   0.828067   \n",
       "5558   0.400114   -0.751519    2.053032    0.364424   0.000000   2.149691   \n",
       "\n",
       "      cluster_17  cluster_8  cluster_12  cluster_3  cluster_9  cluster_5  \\\n",
       "5554         NaN  -0.103407   -0.696322        NaN   0.000000    0.00000   \n",
       "5555    3.068466   0.389782    1.660412   0.565051   0.000000   -1.14595   \n",
       "5556    1.342173   0.373364    0.389803   1.290899   0.000000    0.00000   \n",
       "5557    1.725537  -0.074663    0.726163   1.130658  -0.126752    0.00000   \n",
       "5558    2.461613   1.128856    0.314571  -0.373194   0.129812    0.00000   \n",
       "\n",
       "      cluster_1  cluster_13  cluster_0  cluster_11  cluster_7  cluster_16  \\\n",
       "5554  -1.147162         NaN  -0.244415   -0.130097   0.048848    0.403916   \n",
       "5555   2.688592   -1.166791   1.996624         NaN  -0.351831    1.770652   \n",
       "5556   0.511750    2.654875   1.268348    2.376257  -0.130018         NaN   \n",
       "5557   0.837838   -1.225484   0.491605    1.876478  -4.492730         NaN   \n",
       "5558   0.696582    2.329912  -0.063248         NaN        NaN   -1.082809   \n",
       "\n",
       "      anomaly  \n",
       "5554        0  \n",
       "5555        0  \n",
       "5556        0  \n",
       "5557        0  \n",
       "5558        0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consolidated_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad73af48-25dd-4976-a3ac-f600d0e82ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Assets</th>\n",
       "      <th>EarningsPerShareBasic</th>\n",
       "      <th>NetIncomeLoss</th>\n",
       "      <th>RetainedEarningsAccumulatedDeficit</th>\n",
       "      <th>StockholdersEquity</th>\n",
       "      <th>anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5554</th>\n",
       "      <td>-0.504218</td>\n",
       "      <td>-1.430176</td>\n",
       "      <td>-0.292595</td>\n",
       "      <td>-0.289156</td>\n",
       "      <td>-0.390762</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5555</th>\n",
       "      <td>0.570299</td>\n",
       "      <td>0.447677</td>\n",
       "      <td>1.328116</td>\n",
       "      <td>-0.196763</td>\n",
       "      <td>0.578382</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5556</th>\n",
       "      <td>0.292185</td>\n",
       "      <td>0.743182</td>\n",
       "      <td>0.546734</td>\n",
       "      <td>1.449525</td>\n",
       "      <td>-0.331026</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5557</th>\n",
       "      <td>-0.060695</td>\n",
       "      <td>0.249874</td>\n",
       "      <td>0.205876</td>\n",
       "      <td>-0.390601</td>\n",
       "      <td>-0.420549</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5558</th>\n",
       "      <td>0.102541</td>\n",
       "      <td>0.311040</td>\n",
       "      <td>-0.010839</td>\n",
       "      <td>0.011860</td>\n",
       "      <td>-0.003716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Assets  EarningsPerShareBasic  NetIncomeLoss  \\\n",
       "5554 -0.504218              -1.430176      -0.292595   \n",
       "5555  0.570299               0.447677       1.328116   \n",
       "5556  0.292185               0.743182       0.546734   \n",
       "5557 -0.060695               0.249874       0.205876   \n",
       "5558  0.102541               0.311040      -0.010839   \n",
       "\n",
       "      RetainedEarningsAccumulatedDeficit  StockholdersEquity  anomaly  \n",
       "5554                           -0.289156           -0.390762        0  \n",
       "5555                           -0.196763            0.578382        0  \n",
       "5556                            1.449525           -0.331026        0  \n",
       "5557                           -0.390601           -0.420549        0  \n",
       "5558                            0.011860           -0.003716        0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a96c86f-930d-4f9b-b402-c35a2f10b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_df = consolidated_df.fillna(0)\n",
    "filtered_df = filtered_df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713abd04-f8d2-4dca-aa2e-79b72fd18725",
   "metadata": {},
   "source": [
    "### 2. Train and test split and validations\n",
    "In this section, we will divide our data in train and test splits so we can train and validate our machine learning models' performances and run the last validations before inputting the data in our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dfde6da-00ca-45e2-a8ff-872b71d1f777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in consolidated_df: 654\n",
      "Number of duplicates in filtered_df: 1629\n"
     ]
    }
   ],
   "source": [
    "# Ensure there are no duplicates\n",
    "print(\"Number of duplicates in consolidated_df:\", consolidated_df.duplicated().sum())\n",
    "consolidated_df = consolidated_df.drop_duplicates()\n",
    "# Ensure there are no duplicates\n",
    "print(\"Number of duplicates in filtered_df:\", filtered_df.duplicated().sum())\n",
    "filtered_df = filtered_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3de3023b-1b93-4594-8be4-b1c149058da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Consolidated dataset\n",
    "\n",
    "# Define features and target\n",
    "X_consolidated = consolidated_df.drop(columns=['anomaly'])\n",
    "y_consolidated = consolidated_df['anomaly']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_con, X_test_con, y_train_con, y_test_con = train_test_split(X_consolidated, y_consolidated, test_size=0.3, random_state=42, stratify=y_consolidated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf6dd014-8d0f-4177-9dfe-a39194a2b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Filtered dataset\n",
    "\n",
    "# Define features and target\n",
    "X_filtered = filtered_df.drop(columns=['anomaly'])\n",
    "y_filtered = filtered_df['anomaly']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_filter, X_test_filter, y_train_filter, y_test_filter = train_test_split(X_filtered, y_filtered, test_size=0.3, random_state=42, stratify=y_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9775ca4-23bc-4e35-9323-7cb241485e90",
   "metadata": {},
   "source": [
    "### 3. Modelling\n",
    "\n",
    "In this section, we will try different machine learning models to build our anomaly detection classifier and select the best performer. Initially, we'll start with a Dummy Classifier as a baseline to provide a benchmark for comparison. Following this, we train a IsolationForest, One-Class SVM, Random-Forest classifier.\n",
    "\n",
    "For each model, we'll perform hyperparameter tuning using RandomizedSearchCV to find the best configuration. We evaluate each model using accuracy, confusion matrix, and classification report to understand their performance. Finally, we identify the best performing model based on accuracy and save it to a pickle file for future use. Additionally, we visualize the confusion matrices for the top models to inspect their performance further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3b5748-8280-4d0f-b9e8-693135aa60eb",
   "metadata": {},
   "source": [
    "#### 3.1 Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37124e91-68e4-48ef-8534-2a8019b191bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Classifier:\n",
      "Accuracy: 0.9110054347826086\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1341    0]\n",
      " [ 131    0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      1341\n",
      "           1       0.00      0.00      0.00       131\n",
      "\n",
      "    accuracy                           0.91      1472\n",
      "   macro avg       0.46      0.50      0.48      1472\n",
      "weighted avg       0.83      0.91      0.87      1472\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (1) Consolidated dataset\n",
    "\n",
    "# Instantiate and fit a dummy classifier (e.g., most frequent strategy)\n",
    "dummy_clf = DummyClassifier(strategy='most_frequent')\n",
    "dummy_clf.fit(X_train_con, y_train_con)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_dummy = dummy_clf.predict(X_test_con)\n",
    "\n",
    "print(\"Dummy Classifier:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_con, y_pred_dummy)}\\n\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test_con, y_pred_dummy)}\\n\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test_con, y_pred_dummy, zero_division=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aa0a5e9-b6e7-4043-984d-dd667f5c87bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Classifier:\n",
      "Accuracy: 0.9117896522476675\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1075    0]\n",
      " [ 104    0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      1075\n",
      "           1       0.00      0.00      0.00       104\n",
      "\n",
      "    accuracy                           0.91      1179\n",
      "   macro avg       0.46      0.50      0.48      1179\n",
      "weighted avg       0.83      0.91      0.87      1179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (2) Filtered dataset\n",
    "\n",
    "# Instantiate and fit a dummy classifier (e.g., most frequent strategy)\n",
    "dummy_clf = DummyClassifier(strategy='most_frequent')\n",
    "dummy_clf.fit(X_train_filter, y_train_filter)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_dummy = dummy_clf.predict(X_test_filter)\n",
    "\n",
    "print(\"Dummy Classifier:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_filter, y_pred_dummy)}\\n\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test_filter, y_pred_dummy)}\\n\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test_filter, y_pred_dummy, zero_division=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb1663-448d-4ad2-8ce2-d5c31a595887",
   "metadata": {},
   "source": [
    "#### 3.2 Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78938f71-330a-4080-abbf-420233f23fef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# (1) Consolidated dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Instantiate Isolation Forest\u001b[39;00m\n\u001b[1;32m      4\u001b[0m iso_forest \u001b[38;5;241m=\u001b[39m IsolationForest(contamination\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.09\u001b[39m)  \u001b[38;5;66;03m# Adjust contamination based on your anomaly rate\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43miso_forest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_con\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Predict outliers/anomalies\u001b[39;00m\n\u001b[1;32m      8\u001b[0m y_pred_if \u001b[38;5;241m=\u001b[39m iso_forest\u001b[38;5;241m.\u001b[39mpredict(X_test_con)  \u001b[38;5;66;03m# Anomalies are labeled as -1, normal points as 1\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_iforest.py:285\u001b[0m, in \u001b[0;36mIsolationForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    264\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    Fit estimator.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtree_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;66;03m# Pre-sort indices to avoid that each individual tree of the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;66;03m# ensemble sorts the indices.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m         X\u001b[38;5;241m.\u001b[39msort_indices()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:1049\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1046\u001b[0m     )\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1049\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1058\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "# (1) Consolidated dataset\n",
    "\n",
    "# Instantiate Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.09)  # Adjust contamination based on your anomaly rate\n",
    "iso_forest.fit(X_train_con)\n",
    "\n",
    "# Predict outliers/anomalies\n",
    "y_pred_if = iso_forest.predict(X_test_con)  # Anomalies are labeled as -1, normal points as 1\n",
    "y_pred_if = np.where(y_pred_if == -1, 1, 0)\n",
    "\n",
    "print(\"\\nIsolation Forest:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_con, y_pred_if)}\\n\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test_con, y_pred_if)}\\n\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test_con, y_pred_if, zero_division=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "585b9e09-0ed5-4f85-a51a-ca3b5e5afa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Isolation Forest:\n",
      "Accuracy: 0.833757421543681\n",
      "\n",
      "Confusion Matrix:\n",
      "[[968 107]\n",
      " [ 89  15]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91      1075\n",
      "           1       0.12      0.14      0.13       104\n",
      "\n",
      "    accuracy                           0.83      1179\n",
      "   macro avg       0.52      0.52      0.52      1179\n",
      "weighted avg       0.85      0.83      0.84      1179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (2) Filtered dataset\n",
    "\n",
    "# Instantiate Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.09)  # Adjust contamination based on your anomaly rate\n",
    "iso_forest.fit(X_train_filter)\n",
    "\n",
    "# Predict outliers/anomalies\n",
    "y_pred_if = iso_forest.predict(X_test_filter)  # Anomalies are labeled as -1, normal points as 1\n",
    "y_pred_if = np.where(y_pred_if == -1, 1, 0)\n",
    "\n",
    "print(\"\\nIsolation Forest:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_filter, y_pred_if)}\\n\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test_filter, y_pred_if)}\\n\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test_filter, y_pred_if, zero_division=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a7f95c-06ff-4ea9-b1ce-661eec2403b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85fc14f-d5ff-4edc-a0c7-c43d12c5af7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a8002-43fb-452b-a7fb-316a7eb9b50b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70587036-4349-4d84-8627-1bbff3ee9a3d",
   "metadata": {},
   "source": [
    "1. Logistic regression\n",
    "2. Tree-Based Models: Random forest and Gradient Boosting Machines (GBM): (e.g., XGBoost, LightGBM, CatBoost)\n",
    "3. Support Vector Machines (SVM)\n",
    "4. Neural Networks: MLPs and CNN, LSTMs\n",
    "5. Ensemble Methods\n",
    "6. Anomaly Detection Algorithms: Given the nature of anomalies, consider specialized anomaly detection techniques like Isolation Forest, One-Class SVM, or Autoencoders for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b407b1-06a6-4c7b-bfaf-518ada25a6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
