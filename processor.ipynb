{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c34f7fd23cc5de8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T13:35:59.834135Z",
     "start_time": "2024-06-02T13:35:59.076018600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError, ClientError\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import logging\n",
    "import io\n",
    "import csv\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b9ab8b-6697-4ac5-b0cb-a233f3035fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/zemariatrindade/BTS/Financy_App/Scripts_and_Data/venv/lib/python3.11/site-packages\")\n",
    "\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b8e21-d8eb-4727-a2c8-cf194c26ab11",
   "metadata": {},
   "source": [
    "#### loading variables from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a08ccfd-4e68-4dae-a403-1209aa33f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() \n",
    "\n",
    "aws_access_key_id = os.getenv(\"aws_access_key_id\")\n",
    "aws_secret_access_key = os.getenv(\"aws_secret_access_key\")\n",
    "aws_session_token = os.getenv(\"aws_session_token\")\n",
    "s3_bucket = \"financy\"\n",
    "s3_prefix_path = \"egdar_data/raw/\"\n",
    "\n",
    "s3 = boto3.client(\"s3\",\n",
    "                      aws_access_key_id=aws_access_key_id,\n",
    "                      aws_secret_access_key=aws_secret_access_key,\n",
    "                      aws_session_token=aws_session_token)\n",
    "\n",
    "\n",
    "db_username = os.getenv('db_username')\n",
    "db_password = os.getenv(\"db_password\")\n",
    "db_host = os.getenv(\"db_host\")\n",
    "db_port = os.getenv(\"db_port\")\n",
    "db_name = os.getenv(\"db_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff90f06-6e5b-41cc-8a34-3b33bc21ca2d",
   "metadata": {},
   "source": [
    "#### 1st OPTION : Get results based on a list of tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b18220d-46e4-46fc-8735-383e74a0cb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_tickers_list = pd.read_csv(\"companies_with_anomalies.csv\")[\"Ticker\"].unique().tolist()\n",
    "# TO DO: Move all the files to the 2024-05-31/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e3fce8-8d52-469c-ab8f-c5d68a0dfc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json(list_of_tickers):\n",
    "    list_of_keys = [\"egdar_data/raw/2024-05-31/\" + ticker + \".json\" for ticker in list_of_tickers]\n",
    "    all_records = []  # we get a list of dicts with all the features possible\n",
    "    missing_files = []  # List to keep track of missing files\n",
    "    processed_tickers = []\n",
    "    \n",
    "    # List the objects in the bucket\n",
    "    existing_keys = {obj['Key'] for obj in s3.list_objects_v2(Bucket=s3_bucket, Prefix=\"egdar_data/raw/2024-05-31/\").get('Contents', [])}\n",
    "\n",
    "    for key in list_of_keys:\n",
    "        # Split the string by '/' and take the last part\n",
    "        file_name = key.split('/')[-1]\n",
    "        # Split the last part by '.' and take the first part\n",
    "        ticker = file_name.split('.')[0]\n",
    "\n",
    "        if key not in existing_keys:\n",
    "            missing_files.append(ticker)\n",
    "            continue\n",
    "        else:\n",
    "            processed_tickers.append(ticker)\n",
    "            \n",
    "\n",
    "            # Download JSON file from S3\n",
    "            response = s3.get_object(Bucket=s3_bucket, Key=key)\n",
    "            compressed_data = response[\"Body\"].read()  # not human readable data\n",
    "                \n",
    "            # Decompress the data - in our case the json werent compressed as gzip\n",
    "            #with gzip.GzipFile(fileobj=BytesIO(compressed_data), mode=\"rb\") as gz:\n",
    "             #   decompressed_content = gz.read()  # string dictionary\n",
    "                \n",
    "            # Convert the decompressed content to a dictionary\n",
    "            data = json.loads(compressed_data.decode(\"utf-8\"))  # parsing json content to a dictionary\n",
    "                    \n",
    "            # Check if 'facts' key is present\n",
    "            if 'facts' not in data:\n",
    "                print(f\"No 'facts' key for {ticker}\")\n",
    "                continue\n",
    "    \n",
    "            # Getting the set of units available\n",
    "            reporting_types = list(data[\"facts\"].keys())\n",
    "    \n",
    "            for reporting_type in reporting_types:  # iterating through reporting types\n",
    "                # Getting the set of units available\n",
    "                accounts = list(data[\"facts\"][reporting_type].keys())\n",
    "                \n",
    "                list1 = []\n",
    "                for account in accounts:\n",
    "                    list1 += list(data[\"facts\"][reporting_type][account][\"units\"].keys())\n",
    "                \n",
    "                units_list = list(set(list1))\n",
    "    \n",
    "                # Remember the data is organized by accounts. So for each account (type) we have a lot of records\n",
    "                for acc_name, acc_values in data['facts'][reporting_type].items():  # iterating through accounts\n",
    "                    for unit in units_list:  # iterating through units\n",
    "                        r_data_list = acc_values.get('units', {}).get(unit, {})\n",
    "                        for record in r_data_list:  # iterating through a list of dicts\n",
    "                            record[\"reporting_type\"] = reporting_type\n",
    "                            record[\"units\"] = unit\n",
    "                            record[\"type\"] = acc_name\n",
    "                            record[\"ticker\"] = ticker\n",
    "                            all_records.append(record)\n",
    "\n",
    "    # Select only relevant columns\n",
    "    end_list = []\n",
    "    for record in all_records:\n",
    "        dict1 = {}\n",
    "        dict1[\"end\"] = record.get(\"end\", {})\n",
    "        dict1[\"ticker\"] = record.get(\"ticker\", {})\n",
    "        dict1[\"reporting_type\"] = record.get(\"reporting_type\", {})\n",
    "        dict1[\"form\"] = record.get(\"form\", {})\n",
    "        dict1[\"type\"] = record.get(\"type\", {})\n",
    "        dict1[\"units\"] = record.get(\"units\", {})\n",
    "        dict1[\"val\"] = record.get(\"val\", {})\n",
    "        end_list.append(dict1)\n",
    "    \n",
    "    print(f\"\\nTickers processed: {processed_tickers}\")\n",
    "    print(\"\\nMissing files:\", missing_files)\n",
    "    return end_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac02f12d-9fbc-41c0-9ac0-ca4d85e2c7b4",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- There are a lot of tickers that do not have \"us-gaap\" as their reporting system.\n",
    "- Besides the US-GAAP reporting system we have dei, ifrs-full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "235d58f7-5782-446a-89a9-3e10cdfbf051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_table_inRDS(db_name, db_username, db_password, db_host, db_port):\n",
    "\n",
    "    try:\n",
    "        conn = psycopg2.connect(dbname = db_name, user = db_username, password = db_password, host = db_host, port = db_port)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        sql_query = \"\"\"\n",
    "            DROP TABLE edgar_selected_tickers_table;\n",
    "                    \"\"\"\n",
    "\n",
    "        # Create a new db\n",
    "        cursor.execute(sql_query)\n",
    "\n",
    "        print(\"\\nTable edgar_selected_tickers_table was deleted........\\n\")\n",
    "\n",
    "        conn.commit()\n",
    "\n",
    "        # Close the cursor and connection\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "    except psycopg2.Error as e:\n",
    "        print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "241658bd-fe56-459b-93c0-e05243fac059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_inRDS(db_name, db_username, db_password, db_host, db_port):\n",
    "\n",
    "    try:\n",
    "        conn = psycopg2.connect(dbname = db_name, user = db_username, password = db_password, host = db_host, port = db_port)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        sql_query = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS edgar_selected_tickers_table (\n",
    "            end_date DATE,\n",
    "            ticker TEXT,\n",
    "            reporting_type TEXT,\n",
    "            form TEXT,\n",
    "            type TEXT,\n",
    "            units TEXT,\n",
    "            val REAL\n",
    "        );\n",
    "                    \"\"\"\n",
    "\n",
    "        # Create a new db\n",
    "        cursor.execute(sql_query)\n",
    "\n",
    "        print(\"\\nTable edgar_selected_tickers_table was/is created........\\n\")\n",
    "\n",
    "        conn.commit()\n",
    "\n",
    "        # Close the cursor and connection\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26f321db-5a0d-4491-8503-5f85b7f0f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receives a list of dictionaries in bulk and loads them into aws postgres rds\n",
    "def load_batch_into_database(batch_data, db_name, db_username, db_password, db_host, db_port):\n",
    "    try:\n",
    "\n",
    "        conn = psycopg2.connect(dbname = db_name, user = db_username, password = db_password, host = db_host, port = db_port)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Convert list of dictionaries to CSV format\n",
    "        csv_buffer = io.StringIO()\n",
    "        csv_writer = csv.DictWriter(csv_buffer, fieldnames=batch_data[0].keys())\n",
    "        csv_writer.writeheader()\n",
    "        for record in batch_data:\n",
    "            # Replace empty values with None\n",
    "            record = {key: value if value != '' else None for key, value in record.items()}\n",
    "            csv_writer.writerow(record)\n",
    "        csv_buffer.seek(0)\n",
    "\n",
    "        # Load CSV data into PostgreSQL database using cursor.copy_expert()\n",
    "        copy_sql = \"COPY edgar_selected_tickers_table FROM STDIN WITH CSV HEADER DELIMITER ',' NULL ''\"\n",
    "        cursor.copy_expert(copy_sql, csv_buffer)\n",
    "\n",
    "        # Commit changes and close cursor and connection\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        print(\"\\nbatch data was loaded with success! \\n\")\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd544cd1-65f4-402a-a3d8-5a8a751de2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tickers processed: ['AIG', 'AMNA', 'AMND', 'AMTR', 'AMUB', 'BDCX', 'BDCZ', 'BMY', 'AMJ', 'AAPL', 'BAC', 'BAC-PL', 'BAC-PM', 'BAC-PN', 'BAC-PO', 'BAC-PP', 'BAC-PQ', 'BAC-PS', 'BACRP', 'AMZN', 'AMD', 'BJRI']\n",
      "\n",
      "Missing files: ['AIG-PA', 'FNMA', 'FNMAS', 'FNMAH', 'FNMFN', 'FNMAJ', 'FNMAM', 'FNMAN', 'FNMAI', 'FNMAT', 'FNMAK', 'FNMAL', 'FNMFM', 'FNMAO', 'FNMAG', 'FNMFO', 'FNMAP', 'LUMN', 'WFC', 'WFC-PY', 'WFC-PR', 'WFC-PL', 'WFC-PC', 'WFC-PD', 'WFC-PZ', 'WFCNP', 'WFC-PA', 'DB', 'AGATF', 'DEENF', 'ADZCF', 'OLOXF', 'DGP', 'DGZ', 'DZZ', 'USML', 'WUCT', 'UCIB', 'PFFL', 'QULL', 'SCDL', 'SMHB', 'MLPR', 'MTUL', 'MVRL', 'IWML', 'MLPB', 'HDLB', 'IFED', 'IWDL', 'IWFL', 'ESUS', 'FBGX', 'FEDL', 'CEFD', 'DJCB', 'BMYMP', 'CELG-RI', 'C', 'C-PJ', 'C-PN', 'JPM', 'JPM-PC', 'JPM-PD', 'JPM-PJ', 'JPM-PK', 'JPM-PL', 'JPM-PM', 'GSK', 'GLAXF', 'ATMP', 'AYTEF', 'BALTF', 'BWVTF', 'COWTF', 'VXX', 'VXZ', 'PGMFF', 'SGGFF', 'JJTFF', 'JJUFF', 'PGDDF', 'JJGTF', 'JJMTF', 'JJNTF', 'JJOFF', 'JJPFF', 'JJSSF', 'ICITF', 'JEMTF', 'JJATF', 'JJCTF', 'JJETF', 'DJP', 'GBBEF', 'GRN', 'GRNTF', 'PBR', 'PBR-A', 'JEF', 'HPQ', 'GRPN', 'LKNCY', 'GM', 'MSFT', 'TGT', 'JCI', 'HD', 'LVS', 'BML-PG', 'BML-PH', 'BAC-PE', 'BML-PJ', 'BML-PL', 'BAC-PB', 'BAC-PK', 'MER-PK', 'TSLA', 'GOOGL', 'GOOG', 'INTC', 'IBM', 'PEP', 'SBUX', 'GE', 'NFLX', 'CSCO', 'ORCL', 'QCOM', 'EBAY', 'PYPL', 'SQ', 'BSQKZ', 'LYFT', 'TSN', 'MKC', 'MKC-V', 'SJM', 'KHC', 'MDLZ', 'L', 'YUM', 'DPZ', 'PZZA', 'EAT', 'CAKE', 'RRGB', 'PLAY']\n",
      "\n",
      "Table edgar_selected_tickers_table was deleted........\n",
      "\n",
      "\n",
      "Table edgar_selected_tickers_table was/is created........\n",
      "\n",
      "\n",
      "batch data was loaded with success! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_to_load = process_json(anomaly_tickers_list)\n",
    "delete_table_inRDS(db_name, db_username, db_password, db_host, db_port)\n",
    "create_table_inRDS(db_name, db_username, db_password, db_host, db_port)\n",
    "load_batch_into_database(data_to_load, db_name, db_username, db_password, db_host, db_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a128b-23f5-4c29-96a0-77b3f8abd874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb1c70d-6000-45ad-b3d5-8ae38cfbcf63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9565d406-9400-4d23-9a76-67c14bfe70fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbae2051-8c28-4cf8-aa2f-a530f18a9166",
   "metadata": {},
   "source": [
    "##### 2ND OPTION : Get results based on number of tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e313e7-ac25-46f2-8af1-a9d50b2f3845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of json file tags from s3 bucket - This is to be used only for the 2nd option\n",
    "def get_response_from_s3(s3, s3_bucket, s3_prefix_path):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # List objects with the specified prefix\n",
    "        response = s3.list_objects(Bucket=s3_bucket, Prefix=s3_prefix_path)  # big json file\n",
    "        response_contents = response.get(\"Contents\", [])  # accessing the list \"Contents\"\n",
    "\n",
    "        return response_contents\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e005a6-0419-47a1-8987-f8df330d38c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_tickers = 1\n",
    "\n",
    "objs = get_response_from_s3(s3, s3_bucket, s3_prefix_path)[:number_of_tickers]\n",
    "\n",
    "all_records = [] # we get a list of dicts with all the features possible\n",
    "\n",
    "\n",
    "for obj in objs:\n",
    "    # Extract and print the object keys\n",
    "    file_key = obj[\"Key\"]\n",
    "    \n",
    "    # Split the string by '/' and take the last part\n",
    "    file_name = file_key.split('/')[-1]\n",
    "    # Split the last part by '.' and take the first part\n",
    "    ticker = file_name.split('.')[0]\n",
    "    \n",
    "    # Download JSON file\n",
    "    response = s3.get_object(Bucket=s3_bucket, Key=file_key)  # response metadata\n",
    "    compressed_data = response[\"Body\"].read()  # not human readable data\n",
    "    \n",
    "    # Decompress the data\n",
    "    #with gzip.GzipFile(fileobj=BytesIO(compressed_data), mode=\"rb\") as gz:\n",
    "     #   decompressed_content = gz.read()  # string dictionary\n",
    "    \n",
    "    # Convert the decompressed content to a dictionary\n",
    "    data = json.loads(compressed_data.decode(\"utf-8\"))  # parsing json content to a dictionary\n",
    "\n",
    "\n",
    "    \n",
    "    # Getting the set of units available\n",
    "    accounts = list(data[\"facts\"][\"us-gaap\"].keys())\n",
    "    \n",
    "    list1 = []\n",
    "    for account in accounts:\n",
    "        list1 += list(data[\"facts\"][\"us-gaap\"][account][\"units\"].keys())\n",
    "    \n",
    "    units_list = list(set(list1))\n",
    "\n",
    "\n",
    "    # Check if 'facts' and 'us-gaap' keys are present\n",
    "    if 'facts' not in data or 'us-gaap' not in data['facts']:\n",
    "        print(f\"No 'us-gaap' key for {ticker}\")\n",
    "    \n",
    "    \n",
    "    # Remember the data is organzized by accounts. so for each account(type) we have a lot records\n",
    "    for acc_name, acc_values in data['facts']['us-gaap'].items(): # iterating through accounts\n",
    "        for unit in units_list: # iterating through units\n",
    "            r_data_list = acc_values.get('units', {}).get(unit, {})\n",
    "            for record in r_data_list: # iterating through a list of dicts\n",
    "                record[\"units\"]= unit\n",
    "                record[\"type\"] = acc_name\n",
    "                record[\"ticker\"] = ticker\n",
    "                all_records.append(record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ddaf75-0b88-4f52-8084-378532a61d99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83b4df-6c24-424c-bb19-fb8f0f55a5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
